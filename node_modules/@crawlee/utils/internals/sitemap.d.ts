// @ts-ignore optional peer dependency or compatibility with es2022
/// <reference types="node" />
// @ts-ignore optional peer dependency or compatibility with es2022
/// <reference types="node" />
import { Writable } from 'node:stream';
import type { SAXStream } from 'sax';
type SitemapSource = {
    type: 'url';
    url: string;
} | {
    type: 'raw';
    content: string;
};
declare class ParsingState {
    sources: SitemapSource[];
    urls: string[];
    visitedSitemapUrls: string[];
    context?: 'sitemapindex' | 'urlset';
    loc: boolean;
    resetContext(): void;
}
declare class SitemapTxtParser extends Writable {
    private parsingState;
    private onEnd;
    private decoder;
    private buffer;
    constructor(parsingState: ParsingState, onEnd: () => void);
    private processBuffer;
    _write(chunk: any, _encoding: BufferEncoding, callback: (error?: Error | null | undefined) => void): void;
    _final(callback: (error?: Error | null | undefined) => void): void;
}
/**
 * Loads one or more sitemaps from given URLs, following references in sitemap index files, and exposes the contained URLs.
 *
 * **Example usage:**
 * ```javascript
 * // Load a sitemap
 * const sitemap = await Sitemap.load(['https://example.com/sitemap.xml', 'https://example.com/sitemap_2.xml.gz']);
 *
 * // Enqueue all the contained URLs (including those from sub-sitemaps from sitemap indexes)
 * await crawler.addRequests(sitemap.urls);
 * ```
 */
export declare class Sitemap {
    readonly urls: string[];
    constructor(urls: string[]);
    protected static createXmlParser(parsingState: ParsingState, onEnd: () => void, onError: (error: Error) => void): SAXStream;
    /**
     * Try to load sitemap from the most common locations - `/sitemap.xml` and `/sitemap.txt`.
     * For loading based on `Sitemap` entries in `robots.txt`, the {@apilink RobotsFile} class should be used.
     * @param url The domain URL to fetch the sitemap for.
     * @param proxyUrl A proxy to be used for fetching the sitemap file.
     */
    static tryCommonNames(url: string, proxyUrl?: string): Promise<Sitemap>;
    /**
     * Fetch sitemap content from given URL or URLs and return URLs of referenced pages.
     * @param urls sitemap URL(s)
     * @param proxyUrl URL of a proxy to be used for fetching sitemap contents
     */
    static load(urls: string | string[], proxyUrl?: string): Promise<Sitemap>;
    /**
     * Parse XML sitemap content from a string and return URLs of referenced pages. If the sitemap references other sitemaps, they will be loaded via HTTP.
     * @param content XML sitemap content
     * @param proxyUrl URL of a proxy to be used for fetching sitemap contents
     */
    static fromXmlString(content: string, proxyUrl?: string): Promise<Sitemap>;
    protected static parse(parsingState: ParsingState, proxyUrl?: string): Promise<Sitemap>;
    protected static createParser(resolve: (value: unknown) => void, reject: (value: unknown) => void, parsingState: ParsingState, contentType?: string, url?: URL): SitemapTxtParser | SAXStream;
}
export {};
//# sourceMappingURL=sitemap.d.ts.map